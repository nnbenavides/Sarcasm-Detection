{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Nicholas Benavides, Ray Thai, & Crystal Zheng\n",
    "# Code liberally inspired by and lifted from:\n",
    "# https://github.com/kolchinski/reddit-sarc\n",
    "# https://github.com/cgpotts/cs224u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from itertools import islice, chain\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_dir = '../SARC/2.0/pol'\n",
    "comments_file = os.path.join(pol_dir, 'comments.json')\n",
    "train_file = os.path.join(pol_dir, 'train-balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(comments_file, 'r') as f:\n",
    "    comments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ancestors = []\n",
    "train_responses = []\n",
    "train_labels = []\n",
    "lower = True\n",
    "with open(train_file, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='|')\n",
    "    for row in reader:\n",
    "        ancestors = row[0].split(' ')\n",
    "        responses = row[1].split(' ')\n",
    "        labels = row[2].split(' ')\n",
    "        if lower:\n",
    "            train_ancestors.append([comments[r]['text'].lower() for r in ancestors])\n",
    "            train_responses.append([comments[r]['text'].lower() for r in responses])\n",
    "        else:\n",
    "            train_ancestors.append([comments[r]['text'] for r in ancestors])\n",
    "            train_responses.append([comments[r]['text'] for r in responses])\n",
    "        train_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13630\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "train_vocab = defaultdict(int)\n",
    "for pair in train_responses:\n",
    "    for comment in pair:\n",
    "        for w in nltk.word_tokenize(comment):\n",
    "            train_vocab[w] += 1\n",
    "train_vocab = Counter(train_vocab)\n",
    "print(len(train_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi_c(comment):\n",
    "    return Counter(nltk.word_tokenize(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_phi_r(response_features_pair):\n",
    "    assert len(response_features_pair) == 2\n",
    "    cat = np.concatenate((response_features_pair[0], response_features_pair[1]))\n",
    "    return cat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_phi_c(comment, embeddings):\n",
    "    words = nltk.word_tokenize(comment)\n",
    "    unk = np.zeros(next(iter(embeddings.values())).shape)\n",
    "    return np.sum([embeddings[w] if w in embeddings else unk for w in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_phi_c(comment):\n",
    "    return embed_phi_c(comment, fasttext_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText Embeddings\n",
    "i=0\n",
    "fasttext_lookup = {}\n",
    "with open('../../static/wiki-news-300d-1M-subword.vec') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            x = next(f)\n",
    "        except:\n",
    "            break\n",
    "        try:\n",
    "            fields = x.strip().split()\n",
    "            idx = fields[0]\n",
    "            if idx not in train_vocab: continue\n",
    "            if idx in fasttext_lookup:\n",
    "                print(\"Duplicate! \", idx)\n",
    "            vec = np.array(fields[1:], dtype=np.float32)\n",
    "            fasttext_lookup[idx] = vec\n",
    "            i += 1\n",
    "            #if i%500 == 0: print(i)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "#print(len(fasttext_lookup))\n",
    "#print(type(fasttext_lookup['the']), fasttext_lookup['the'].shape, sum(fasttext_lookup['the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = train_responses\n",
    "phi_c = unigrams_phi_c\n",
    "N = len(responses)\n",
    "feat_dicts = [[],[]]\n",
    "for i in range(N):\n",
    "    assert len(responses[i]) == 2\n",
    "    feat_dicts[0].append(phi_c(responses[i][0]))\n",
    "    feat_dicts[1].append(phi_c(responses[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe Embeddings\n",
    "i=0\n",
    "glove_lookup = {}\n",
    "with open('../../static/glove/glove.6B.300d.txt') as f:\n",
    "#with open('../../static/') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            x = next(f)\n",
    "        except:\n",
    "            break\n",
    "        try:\n",
    "            fields = x.strip().split()\n",
    "            idx = fields[0]\n",
    "            if idx not in train_vocab: continue\n",
    "            if idx in glove_lookup:\n",
    "                print(\"Duplicate! \", idx)\n",
    "            vec = np.array(fields[1:], dtype=np.float32)\n",
    "            glove_lookup[idx] = vec\n",
    "            i += 1\n",
    "            #if i%500 == 0: print(i)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "#print(len(glove_lookup))\n",
    "#print(type(glove_lookup['the']), glove_lookup['the'].shape, sum(glove_lookup['the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phi_c turns comments into features\n",
    "#phi_a combines ancestor features into summary\n",
    "#phi_r combines response features into summary\n",
    "#Note that this is for the \"balanced\" framing!\n",
    "#TODO: Initially ignoring ancestors, include them as another vector later\n",
    "def build_dataset(ancestors, responses, labels, phi_c, phi_a, phi_r, vectorizer=None, vectorize = True):\n",
    "    X = []\n",
    "    Y = []\n",
    "    feat_dicts = [[],[]]\n",
    "    N = len(ancestors)\n",
    "    assert N == len(responses) == len(labels)\n",
    "    print(N)\n",
    "    for i in range(N):\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print(i)\n",
    "        assert len(responses[i]) == 2\n",
    "        feat_dicts[0].append(phi_c(responses[i][0]))\n",
    "        feat_dicts[1].append(phi_c(responses[i][1]))\n",
    "    \n",
    "        #We only care about the first of the two labels since in the balanced setting\n",
    "        #they're either 0 1 or 1 0\n",
    "        Y.append(int(labels[i][0]))\n",
    "            \n",
    "    if vectorize:\n",
    "        # In training, we want a new vectorizer:\n",
    "        if vectorizer == None:\n",
    "            vectorizer = DictVectorizer(sparse=False)\n",
    "            #print(feat_dicts[0][:10], feat_dicts[1][:10])\n",
    "            feat_matrix = vectorizer.fit_transform(feat_dicts[0] + feat_dicts[1])\n",
    "        # In assessment, we featurize using the existing vectorizer:\n",
    "        else:\n",
    "            feat_matrix = vectorizer.transform(chain(feat_dicts[0], feat_dicts[1]))\n",
    "        \n",
    "        response_pair_feats = [feat_matrix[:N], feat_matrix[N:]]\n",
    "    else:\n",
    "        response_pair_feats = feat_dicts\n",
    "        #print(response_pair_feats[0])\n",
    "\n",
    "    #assert len(feat_matrix == 2*N) \n",
    "    #print((feat_matrix[0]), len(feat_matrix[1]))\n",
    "    \n",
    "    X = [phi_r((response_pair_feats[0][i], response_pair_feats[1][i])) for i in range(N)]\n",
    "    #X = list(map(phi_r, response_pair_feats))\n",
    "    \n",
    "    return {'X': np.array(X),\n",
    "            'y': np.array(Y),\n",
    "            'vectorizer': vectorizer,\n",
    "            'raw_examples': (ancestors, responses)}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xval_model(model_fit_fn, X, y, folds):\n",
    "    kf = KFold(folds)\n",
    "    macro_f1_avg = 0\n",
    "    for train, test in kf.split(X, y):\n",
    "        model = model_fit_fn(X[train], y[train])\n",
    "        predictions = model.predict(X[test])\n",
    "        report = classification_report(y[test], predictions, output_dict = True)\n",
    "        macro_f1_avg += report['macro avg']['f1-score']\n",
    "        print(classification_report(y[test], predictions, digits=3))\n",
    "    macro_f1_avg /= folds\n",
    "    output = 'Average Macro F1 Score across folds = ' + str(macro_f1_avg)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram_dataset = build_dataset(train_ancestors, train_responses, train_labels, unigrams_phi_c, None, concat_phi_r)\n",
    "#unigram_dataset['X'].shape\n",
    "#np.save('pol-balanced-unigram-X.npy', unigram_dataset['X'])\n",
    "#np.save('pol-balanced-unigram-y.npy', unigram_dataset['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6834\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6834, 600)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''fasttext_dataset = build_dataset(\n",
    "    train_ancestors, train_responses, train_labels, fasttext_phi_c, None, concat_phi_r, None, False)\n",
    "\n",
    "fasttext_dataset['X'].shape\n",
    "np.save('pol-balanced-fasttext-X.npy', fasttext_dataset['X'])\n",
    "np.save('pol-balanced-fasttext-y.npy', fasttext_dataset['y'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_phi_c(comment):\n",
    "    return embed_phi_c(comment, glove_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"glove_dataset = build_dataset(\\n    train_ancestors, train_responses, train_labels, glove_phi_c, None, concat_phi_r, None, False)\\n\\nfasttext_dataset['X'].shape\\nnp.save('pol-balanced-glove-X.npy', glove_dataset['X'])\\nnp.save('pol-balanced-glove-y.npy', glove_dataset['y'])\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''glove_dataset = build_dataset(\n",
    "    train_ancestors, train_responses, train_labels, glove_phi_c, None, concat_phi_r, None, False)\n",
    "\n",
    "fasttext_dataset['X'].shape\n",
    "np.save('pol-balanced-glove-X.npy', glove_dataset['X'])\n",
    "np.save('pol-balanced-glove-y.npy', glove_dataset['y'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELMo Embeddings\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_phi_c(comment):\n",
    "    vecs = elmo.embed_sentence(nltk.word_tokenize(comment))\n",
    "    elmo_avg_vec = vecs.mean(axis = 0)\n",
    "    return elmo_avg_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"elmo_dataset = build_dataset(\\n    train_ancestors, train_responses, train_labels, elmo_phi_c, None, concat_phi_r, None, False)\\nnp.save('pol-balanced-elmo-X.npy', elmo_dataset['X'])\\nnp.save('pol-balanced-elmo-y.npy', elmo_dataset['y'])\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''elmo_dataset = build_dataset(\n",
    "    train_ancestors, train_responses, train_labels, elmo_phi_c, None, concat_phi_r, None, False)\n",
    "np.save('pol-balanced-elmo-X.npy', elmo_dataset['X'])\n",
    "np.save('pol-balanced-elmo-y.npy', elmo_dataset['y'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_basic_rnn(X, y):  \n",
    "    mod = TorchShallowNeuralClassifier(hidden_dim = 200, max_iter = 100)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.008228679187595844"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.680     0.671     0.676       681\n",
      "           1      0.678     0.687     0.682       686\n",
      "\n",
      "   micro avg      0.679     0.679     0.679      1367\n",
      "   macro avg      0.679     0.679     0.679      1367\n",
      "weighted avg      0.679     0.679     0.679      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.008335313992574811"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.697     0.669     0.683       708\n",
      "           1      0.659     0.687     0.673       659\n",
      "\n",
      "   micro avg      0.678     0.678     0.678      1367\n",
      "   macro avg      0.678     0.678     0.678      1367\n",
      "weighted avg      0.679     0.678     0.678      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.00953952653799206"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.662     0.669     0.665       679\n",
      "           1      0.670     0.663     0.666       688\n",
      "\n",
      "   micro avg      0.666     0.666     0.666      1367\n",
      "   macro avg      0.666     0.666     0.666      1367\n",
      "weighted avg      0.666     0.666     0.666      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.009207124589011073"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.679     0.693     0.686       680\n",
      "           1      0.689     0.675     0.682       687\n",
      "\n",
      "   micro avg      0.684     0.684     0.684      1367\n",
      "   macro avg      0.684     0.684     0.684      1367\n",
      "weighted avg      0.684     0.684     0.684      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.015857665915973485"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.695     0.672     0.683       690\n",
      "           1      0.676     0.698     0.687       676\n",
      "\n",
      "   micro avg      0.685     0.685     0.685      1366\n",
      "   macro avg      0.685     0.685     0.685      1366\n",
      "weighted avg      0.686     0.685     0.685      1366\n",
      "\n",
      "Average Macro F1 Score across folds = 0.6783479494238109\n"
     ]
    }
   ],
   "source": [
    "#TorchShallowNeural Classifier w/ ELMo Embeddings\n",
    "elmo_X = np.load('pol-balanced-elmo-X.npy')\n",
    "elmo_y = np.load('pol-balanced-elmo-y.npy')\n",
    "xval_model(fit_basic_rnn, elmo_X, elmo_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 7.7864465310995e-0558"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.685     0.699     0.692       681\n",
      "           1      0.695     0.681     0.688       686\n",
      "\n",
      "   micro avg      0.690     0.690     0.690      1367\n",
      "   macro avg      0.690     0.690     0.690      1367\n",
      "weighted avg      0.690     0.690     0.690      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 7.636411601197324e-05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.692     0.702     0.697       708\n",
      "           1      0.675     0.665     0.670       659\n",
      "\n",
      "   micro avg      0.684     0.684     0.684      1367\n",
      "   macro avg      0.684     0.683     0.683      1367\n",
      "weighted avg      0.684     0.684     0.684      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 7.300523066078313e-05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.674     0.677     0.676       679\n",
      "           1      0.680     0.677     0.679       688\n",
      "\n",
      "   micro avg      0.677     0.677     0.677      1367\n",
      "   macro avg      0.677     0.677     0.677      1367\n",
      "weighted avg      0.677     0.677     0.677      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 6.793935790483374e-05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.664     0.688     0.676       680\n",
      "           1      0.680     0.655     0.667       687\n",
      "\n",
      "   micro avg      0.672     0.672     0.672      1367\n",
      "   macro avg      0.672     0.672     0.671      1367\n",
      "weighted avg      0.672     0.672     0.671      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 7.507914324378362e-05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.684     0.681     0.683       690\n",
      "           1      0.676     0.679     0.677       676\n",
      "\n",
      "   micro avg      0.680     0.680     0.680      1366\n",
      "   macro avg      0.680     0.680     0.680      1366\n",
      "weighted avg      0.680     0.680     0.680      1366\n",
      "\n",
      "Average Macro F1 Score across folds = 0.6804302496699\n"
     ]
    }
   ],
   "source": [
    "#TorchShallowNeural Classifier w/ Unigram Features\n",
    "unigram_X = np.load('pol-balanced-unigram-X.npy')\n",
    "unigram_y = np.load('pol-balanced-unigram-y.npy')\n",
    "xval_model(fit_basic_rnn, unigram_X, unigram_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.09518670104444027"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.691     0.686     0.688       681\n",
      "           1      0.690     0.695     0.693       686\n",
      "\n",
      "   micro avg      0.691     0.691     0.691      1367\n",
      "   macro avg      0.691     0.691     0.691      1367\n",
      "weighted avg      0.691     0.691     0.691      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.10693638026714325"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.699     0.710     0.704       708\n",
      "           1      0.683     0.671     0.677       659\n",
      "\n",
      "   micro avg      0.691     0.691     0.691      1367\n",
      "   macro avg      0.691     0.691     0.691      1367\n",
      "weighted avg      0.691     0.691     0.691      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.10639485903084278"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.683     0.679     0.681       679\n",
      "           1      0.685     0.689     0.687       688\n",
      "\n",
      "   micro avg      0.684     0.684     0.684      1367\n",
      "   macro avg      0.684     0.684     0.684      1367\n",
      "weighted avg      0.684     0.684     0.684      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.09764285106211901"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.682     0.662     0.672       680\n",
      "           1      0.675     0.694     0.684       687\n",
      "\n",
      "   micro avg      0.678     0.678     0.678      1367\n",
      "   macro avg      0.678     0.678     0.678      1367\n",
      "weighted avg      0.678     0.678     0.678      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.10250043030828238"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.725     0.691     0.708       690\n",
      "           1      0.699     0.732     0.715       676\n",
      "\n",
      "   micro avg      0.712     0.712     0.712      1366\n",
      "   macro avg      0.712     0.712     0.712      1366\n",
      "weighted avg      0.712     0.712     0.711      1366\n",
      "\n",
      "Average Macro F1 Score across folds = 0.6909389446517598\n"
     ]
    }
   ],
   "source": [
    "#TorchShallowNeural Classifier w/ FastText Embeddings\n",
    "fasttext_X = np.load('pol-balanced-fasttext-X.npy')\n",
    "fasttext_y = np.load('pol-balanced-fasttext-y.npy')\n",
    "xval_model(fit_basic_rnn, fasttext_X, fasttext_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1 of 100; error is 12.58146500587463487"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.670     0.671     0.671       681\n",
      "           1      0.673     0.672     0.673       686\n",
      "\n",
      "   micro avg      0.672     0.672     0.672      1367\n",
      "   macro avg      0.672     0.672     0.672      1367\n",
      "weighted avg      0.672     0.672     0.672      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 1.5711117088794708"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.698     0.637     0.666       708\n",
      "           1      0.644     0.704     0.672       659\n",
      "\n",
      "   micro avg      0.669     0.669     0.669      1367\n",
      "   macro avg      0.671     0.671     0.669      1367\n",
      "weighted avg      0.672     0.669     0.669      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 1.8345576524734497"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.670     0.689     0.680       679\n",
      "           1      0.685     0.666     0.675       688\n",
      "\n",
      "   micro avg      0.677     0.677     0.677      1367\n",
      "   macro avg      0.678     0.677     0.677      1367\n",
      "weighted avg      0.678     0.677     0.677      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.9789055436849594"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.644     0.626     0.635       680\n",
      "           1      0.640     0.658     0.649       687\n",
      "\n",
      "   micro avg      0.642     0.642     0.642      1367\n",
      "   macro avg      0.642     0.642     0.642      1367\n",
      "weighted avg      0.642     0.642     0.642      1367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.5511099845170975"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.681     0.667     0.674       690\n",
      "           1      0.667     0.682     0.674       676\n",
      "\n",
      "   micro avg      0.674     0.674     0.674      1366\n",
      "   macro avg      0.674     0.674     0.674      1366\n",
      "weighted avg      0.674     0.674     0.674      1366\n",
      "\n",
      "Average Macro F1 Score across folds = 0.666924463169167\n"
     ]
    }
   ],
   "source": [
    "#TorchShallowNeural Classifier w/ GloVe Embeddings\n",
    "glove_X = np.load('pol-balanced-glove-X.npy')\n",
    "glove_y = np.load('pol-balanced-glove-y.npy')\n",
    "xval_model(fit_basic_rnn, glove_X, glove_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_torch_rnn(X, y):  \n",
    "    mod = mod = TorchRNNClassifier(\n",
    "        train_vocab,\n",
    "        embedding = None,\n",
    "        max_iter=50,\n",
    "        bidirectional=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'$UNK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-487347356f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_torch_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melmo_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melmo_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-64ee9d3b1dde>\u001b[0m in \u001b[0;36mxval_model\u001b[0;34m(model_fit_fn, X, y, folds)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmacro_f1_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-829d951e2d88>\u001b[0m in \u001b[0;36mfit_torch_rnn\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         bidirectional=True)\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Senior/CS224U/project/reddit-sarc/notebooks/torch_rnn_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mclass2index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         dataloader = torch.utils.data.DataLoader(\n\u001b[1;32m    208\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Senior/CS224U/project/reddit-sarc/notebooks/torch_rnn_classifier.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTorchRNNDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Senior/CS224U/project/reddit-sarc/notebooks/torch_rnn_classifier.py\u001b[0m in \u001b[0;36m_prepare_dataset\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0munk_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'$UNK'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '$UNK'"
     ]
    }
   ],
   "source": [
    "xval_model(fit_torch_rnn, elmo_dataset['X'], elmo_dataset['y'], 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
